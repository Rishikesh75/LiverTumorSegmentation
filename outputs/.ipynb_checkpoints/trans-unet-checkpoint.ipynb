{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore # type: ignore\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "2.0.2\n",
      "2.18.0\n",
      "2.6.0+cpu\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n",
    "print(torch.__version__)\n",
    "print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: train_folder\\volume-0.nii\n",
      "Loading file: train_folder\\Segmentation\\segmentation-0.nii\n",
      "Loading file: train_folder\\volume-1.nii\n",
      "Loading file: train_folder\\Segmentation\\segmentation-1.nii\n",
      "Loading file: train_folder\\volume-2.nii\n",
      "Loading file: train_folder\\Segmentation\\segmentation-2.nii\n",
      "Loading file: train_folder\\volume-3.nii\n",
      "Loading file: train_folder\\Segmentation\\segmentation-3.nii\n",
      "Loading file: train_folder\\volume-4.nii\n",
      "Loading file: train_folder\\Segmentation\\segmentation-4.nii\n",
      "Training data shape: (2090, 128, 128)\n",
      "Segmentation mask shape: (2090, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "VOLUME_DIR = \"train_folder\"\n",
    "SEGMENTATION_DIR = os.path.join(\"train_folder\", \"Segmentation\")\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"Load a NIfTI file and return the NumPy array\"\"\"\n",
    "    print(f\"Loading file: {file_path}\") \n",
    "    nifti_img = nib.load(file_path) \n",
    "    return nifti_img.get_fdata()\n",
    "\n",
    "def preprocess_data(volume_path, segmentation_path):\n",
    "    \"\"\"Load and preprocess volume and segmentation data\"\"\"\n",
    "    volume_data = load_nifti(volume_path)\n",
    "    segmentation_data = load_nifti(segmentation_path).astype(int)\n",
    "\n",
    "    volume_data = (volume_data - np.min(volume_data)) / (np.max(volume_data) - np.min(volume_data))\n",
    "\n",
    "    volume_resized = np.array([\n",
    "        resize(slice, IMG_SIZE, mode='constant', preserve_range=True) \n",
    "        for slice in volume_data.transpose(2, 0, 1)\n",
    "    ])\n",
    "    segmentation_resized = np.array([\n",
    "        resize(slice, IMG_SIZE, mode='constant', preserve_range=True, order=0) \n",
    "        for slice in segmentation_data.transpose(2, 0, 1)\n",
    "    ])\n",
    "\n",
    "    segmentation_onehot = to_categorical(segmentation_resized, num_classes=3)\n",
    "\n",
    "    return volume_resized, segmentation_onehot\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(VOLUME_DIR) if f.endswith(\".nii\")])\n",
    "mask_files = sorted([f for f in os.listdir(SEGMENTATION_DIR) if f.endswith(\".nii\")])\n",
    "\n",
    "for img_file, mask_file in zip(image_files, mask_files):\n",
    "    volume_path = os.path.join(VOLUME_DIR, img_file)\n",
    "    segmentation_path = os.path.join(SEGMENTATION_DIR, mask_file)\n",
    "    vol, seg = preprocess_data(volume_path, segmentation_path)\n",
    "    X_train.append(vol)\n",
    "    Y_train.append(seg)\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Segmentation mask shape:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: test_folder\\volume-10.nii\n",
      "Loading: test_folder\\segmentation\\segmentation-10.nii\n",
      "Final Test Data Shape: (501, 128, 128, 1)\n",
      "Final Test Mask Shape: (501, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "# Function to load NIfTI files\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"Load a NIfTI file and return the NumPy array\"\"\"\n",
    "    print(f\"Loading: {file_path}\")  # Debugging: Print file being loaded\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    nifti_img = nib.load(file_path)\n",
    "    return nifti_img.get_fdata()\n",
    "\n",
    "# Function to preprocess test data\n",
    "def preprocess_data(test_volume_path, test_segmentation_path):\n",
    "    \"\"\"Load and preprocess a single test volume and its corresponding segmentation mask\"\"\"\n",
    "    volume_data = load_nifti(test_volume_path)\n",
    "    segmentation_data = load_nifti(test_segmentation_path).astype(int)\n",
    "\n",
    "    # Normalize CT images (0-1)\n",
    "    volume_data = (volume_data - np.min(volume_data)) / (np.max(volume_data) - np.min(volume_data))\n",
    "\n",
    "    # Resize to (128, 128)\n",
    "    from skimage.transform import resize\n",
    "    IMG_SIZE = (128, 128)\n",
    "    \n",
    "    volume_resized = np.array([\n",
    "        resize(slice, IMG_SIZE, mode='constant', preserve_range=True)\n",
    "        for slice in volume_data.transpose(2, 0, 1)  # Rearrange to (slices, H, W)\n",
    "    ])\n",
    "    \n",
    "    segmentation_resized = np.array([\n",
    "        resize(slice, IMG_SIZE, mode='constant', preserve_range=True, order=0) \n",
    "        for slice in segmentation_data.transpose(2, 0, 1)\n",
    "    ])\n",
    "\n",
    "    # One-hot encode segmentation masks (background=0, liver=1, tumor=2)\n",
    "    \n",
    "    segmentation_onehot = to_categorical(segmentation_resized, num_classes=3)\n",
    "\n",
    "    return volume_resized, segmentation_onehot\n",
    "\n",
    "# ---- Load Test Data ----\n",
    "test_image_path = os.path.join(\"test_folder\") \n",
    "test_mask_path = os.path.join(\"test_folder\", \"segmentation\") \n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(test_image_path) or not os.path.exists(test_mask_path):\n",
    "    raise FileNotFoundError(\"Test data folder or segmentation folder not found!\")\n",
    "\n",
    "# Get sorted filenames\n",
    "image_files = sorted([f for f in os.listdir(test_image_path) if f.endswith(\".nii\")])\n",
    "mask_files = sorted([f for f in os.listdir(test_mask_path) if f.endswith(\".nii\")])\n",
    "\n",
    "# Check if the number of images and masks match\n",
    "if len(image_files) != len(mask_files):\n",
    "    raise ValueError(\"Mismatch between the number of test images and segmentation masks!\")\n",
    "\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "# Load and preprocess each test image and mask\n",
    "for img_file, mask_file in zip(image_files, mask_files):\n",
    "    volume_path = os.path.join(test_image_path, img_file)  # Correct path joining\n",
    "    segmentation_path = os.path.join(test_mask_path, mask_file)  # Correct path joining\n",
    "\n",
    "    vol, seg = preprocess_data(volume_path, segmentation_path)\n",
    "    X_test.append(vol)\n",
    "    Y_test.append(seg)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_test = np.array(X_test)  # Shape: (num_volumes, num_slices, 128, 128)\n",
    "Y_test = np.array(Y_test)  # Shape: (num_volumes, num_slices, 128, 128, 3)\n",
    "\n",
    "# Reshape to flatten across all slices\n",
    "X_test = X_test.reshape(-1, 128, 128, 1)  # Add channel dimension\n",
    "Y_test = Y_test.reshape(-1, 128, 128, 3)  # Keep segmentation masks in one-hot format\n",
    "\n",
    "# Print shapes\n",
    "print(\"Final Test Data Shape:\", X_test.shape)  # (total_slices, 128, 128, 1)\n",
    "print(\"Final Test Mask Shape:\", Y_test.shape)  # (total_slices, 128, 128, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, in_channels=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, n_patches+1, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, depth=4, num_heads=8, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim*mlp_ratio), batch_first=True)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, in_channels=1, out_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.transformer = TransformerEncoder(embed_dim=embed_dim)\n",
    "\n",
    "        self.decoder_dim = 256\n",
    "        self.linear_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, self.decoder_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.decoder_dim, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, out_channels, kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # (B, n_patches+1, embed_dim)\n",
    "        x = self.transformer(x)  # (B, n_patches+1, embed_dim)\n",
    "        x = x[:, 1:, :]  # Remove cls token, shape: (B, n_patches, embed_dim)\n",
    "\n",
    "        # Reshape to 2D feature map\n",
    "        h = w = int(math.sqrt(x.shape[1]))\n",
    "        x = self.linear_decoder(x)  # (B, n_patches, decoder_dim)\n",
    "        x = x.permute(0, 2, 1).contiguous().view(B, self.decoder_dim, h, w)  # (B, decoder_dim, H, W)\n",
    "\n",
    "        x = self.upsample(x)  # (B, out_channels, 128, 128)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Assuming x_train is (2090, 128, 128) and y_train is (2090, 128, 128, 3)\n",
    "class SliceDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        # Convert to float32 and torch tensors\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)  # (N, 1, 128, 128)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).permute(0, 3, 1, 2)  # (N, 3, 128, 128)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and loader\n",
    "batch_size = 16\n",
    "dataset = SliceDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch from the DataLoader and print details\n",
    "for images, masks in train_loader:\n",
    "    print(\"✅ Images shape:\", images.shape)  # Should be (16, 1, 128, 128)\n",
    "    print(\"✅ Masks shape:\", masks.shape)    # Should be (16, 3, 128, 128)\n",
    "    \n",
    "    # Check value ranges\n",
    "    print(\"Image min/max:\", images.min().item(), images.max().item())\n",
    "    print(\"Mask unique values (flattened):\", torch.unique(masks))\n",
    "\n",
    "    # Optional: check one-hot encoding validity\n",
    "    is_one_hot = torch.all((masks.sum(dim=1) == 1) | (masks.sum(dim=1) == 0))\n",
    "    print(\"Is one-hot encoded?\", is_one_hot.item())\n",
    "\n",
    "    break  # Just check the first batch\n",
    "\n",
    "# Test DataLoader\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Image batch shape: {images.shape}\")  # Should be (16, 1, 128, 128)\n",
    "    print(f\"Mask batch shape: {masks.shape}\")    # Should be (16, 3, 128, 128)\n",
    "    \n",
    "    # Optionally test forward pass through your model\n",
    "    model = TransUNet(in_channels=1, out_channels=3)  # Ensure model is defined\n",
    "    outputs = model(images)  # Forward pass\n",
    "    print(f\"Model output shape: {outputs.shape}\")  # Should be (16, 3, 128, 128)\n",
    "    \n",
    "    break  # Just check one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Loss: 0.8761\n",
      "Epoch [2/15] Loss: 0.7868\n",
      "Epoch [3/15] Loss: 0.7869\n",
      "Epoch [4/15] Loss: 0.7866\n",
      "Epoch [5/15] Loss: 0.7886\n",
      "Epoch [6/15] Loss: 0.7867\n",
      "Epoch [7/15] Loss: 0.7891\n",
      "Epoch [8/15] Loss: 0.7863\n",
      "Epoch [9/15] Loss: 0.7863\n",
      "Epoch [10/15] Loss: 0.7859\n",
      "Epoch [11/15] Loss: 0.7875\n",
      "Epoch [12/15] Loss: 0.7864\n",
      "Epoch [13/15] Loss: 0.7869\n",
      "Epoch [14/15] Loss: 0.7874\n",
      "Epoch [15/15] Loss: 0.7858\n",
      "\n",
      "Best Epoch: 15 with Loss: 0.7858\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dice Loss implementation for multi-class segmentation\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (B, C, H, W), targets: (B, H, W)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.shape[1]).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * targets_one_hot, dims)\n",
    "        union = torch.sum(probs + targets_one_hot, dims)\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "# === Setup ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TransUNet().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "dice_loss = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save directory\n",
    "save_dir = \"checkpoints/transunet_experiment\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "log_file = os.path.join(save_dir, \"training_log.txt\")\n",
    "\n",
    "# === Training Loop with per-epoch saving and best-epoch tracking ===\n",
    "num_epochs = 15\n",
    "best_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "with open(log_file, \"w\") as f:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_idx = torch.argmax(targets, dim=1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = ce_loss(outputs, targets_idx) + dice_loss(outputs, targets_idx)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Save model every epoch\n",
    "        model_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Check if best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        # Print and log\n",
    "        log_msg = f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\"\n",
    "        print(log_msg, flush=True)  # Ensures the message is printed immediately\n",
    "        f.write(log_msg + \"\\n\")\n",
    "\n",
    "    final_msg = f\"\\nBest Epoch: {best_epoch} with Loss: {best_loss:.4f}\"\n",
    "    print(final_msg, flush=True)  # Prints the final best epoch message immediately\n",
    "    f.write(final_msg + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Data Shape: (501, 128, 128)\n",
      "Final Test Mask Shape: (501, 128, 128, 3)\n",
      "Training data shape: (2090, 128, 128)\n",
      "Segmentation mask shape: (2090, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes\n",
    "X_test = np.squeeze(X_test, axis=-1)  # Now shape: (501, 128, 128)\n",
    "print(\"Final Test Data Shape:\", X_test.shape)  # (total_slices, 128, 128, 1)\n",
    "print(\"Final Test Mask Shape:\", Y_test.shape)  # (total_slices, 128, 128, 3)\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Segmentation mask shape:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed input shape: torch.Size([16, 1, 128, 128])\n",
      "Fixed mask shape: torch.Size([16, 3, 128, 128])\n",
      "Model output shape: torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "class TestSliceDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        # Remove the last dimension if it's singleton (1)\n",
    "        if x.shape[-1] == 1:\n",
    "            x = np.squeeze(x, axis=-1)  # Now (N, 128, 128)\n",
    "\n",
    "        self.x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)  # (N, 1, 128, 128)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).permute(0, 3, 1, 2)  # (N, 3, 128, 128)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Use it\n",
    "test_dataset = TestSliceDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "for images, masks in test_loader:\n",
    "    print(\"Fixed input shape:\", images.shape)  # ✅ Should be (16, 1, 128, 128)\n",
    "    print(\"Fixed mask shape:\", masks.shape)    # ✅ Should be (16, 3, 128, 128)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    print(\"Model output shape:\", outputs.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recreate the model architecture first\n",
    "# model = UNetPlusPlus(in_channels=1, out_channels=3)\n",
    "\n",
    "# # Load the weights\n",
    "# model.load_state_dict(torch.load('unetplusplus_epoch_15.pth'))\n",
    "\n",
    "# # Put it in eval mode if you're using it for inference\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dice Score on Test Set: 0.7775\n"
     ]
    }
   ],
   "source": [
    "def dice_score(preds, targets, epsilon=1e-6):\n",
    "    # Assumes preds are one-hot or softmax outputs (batch, C, H, W)\n",
    "    preds = torch.argmax(preds, dim=1)  # (batch, H, W)\n",
    "    targets = torch.argmax(targets, dim=1)  # (batch, H, W)\n",
    "\n",
    "    dice_total = 0\n",
    "    for cls in range(3):  # For each class\n",
    "        pred_cls = (preds == cls).float()\n",
    "        target_cls = (targets == cls).float()\n",
    "        \n",
    "        intersection = (pred_cls * target_cls).sum()\n",
    "        union = pred_cls.sum() + target_cls.sum()\n",
    "        \n",
    "        dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "        dice_total += dice\n",
    "\n",
    "    return dice_total / 3  # Average over 3 classes\n",
    "\n",
    "total_dice = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        outputs = model(images)  # Forward pass\n",
    "        batch_dice = dice_score(outputs, masks)\n",
    "        total_dice += batch_dice.item()\n",
    "        num_batches += 1\n",
    "\n",
    "avg_dice = total_dice / num_batches\n",
    "print(f\"Average Dice Score on Test Set: {avg_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score_per_class(preds, targets, epsilon=1e-6):\n",
    "    # Convert from softmax/one-hot to label maps\n",
    "    preds = torch.argmax(preds, dim=1)    # (batch, H, W)\n",
    "    targets = torch.argmax(targets, dim=1)  # (batch, H, W)\n",
    "\n",
    "    class_dice_scores = []\n",
    "\n",
    "    for cls in range(3):  # 3 classes\n",
    "        pred_cls = (preds == cls).float()\n",
    "        target_cls = (targets == cls).float()\n",
    "\n",
    "        intersection = (pred_cls * target_cls).sum()\n",
    "        union = pred_cls.sum() + target_cls.sum()\n",
    "\n",
    "        dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "        class_dice_scores.append(dice.item())\n",
    "\n",
    "    return class_dice_scores  # List: [dice_class_0, dice_class_1, dice_class_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dice Score for Class 0: 0.9888\n",
      "Average Dice Score for Class 1: 0.6250\n",
      "Average Dice Score for Class 2: 0.7188\n"
     ]
    }
   ],
   "source": [
    "total_dice = [0.0, 0.0, 0.0]\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        outputs = model(images)\n",
    "        dice_scores = dice_score_per_class(outputs, masks)\n",
    "        for i in range(3):\n",
    "            total_dice[i] += dice_scores[i]\n",
    "        num_batches += 1\n",
    "\n",
    "avg_dice_per_class = [d / num_batches for d in total_dice]\n",
    "for i, score in enumerate(avg_dice_per_class):\n",
    "    print(f\"Average Dice Score for Class {i}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dice Score for Foreground (Classes 1 & 2): 0.6719\n"
     ]
    }
   ],
   "source": [
    "def dice_score_foreground(preds, targets, epsilon=1e-6):\n",
    "    preds = torch.argmax(preds, dim=1)  # (B, H, W)\n",
    "    targets = torch.argmax(targets, dim=1)  # (B, H, W)\n",
    "\n",
    "    dice_scores = []\n",
    "\n",
    "    for cls in [1, 2]:  # Only foreground classes\n",
    "        pred_cls = (preds == cls).float()\n",
    "        target_cls = (targets == cls).float()\n",
    "        \n",
    "        intersection = (pred_cls * target_cls).sum()\n",
    "        union = pred_cls.sum() + target_cls.sum()\n",
    "        \n",
    "        dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    return sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "# Compute combined Dice for foreground (classes 1 and 2)\n",
    "total_dice_fg = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        outputs = model(images)\n",
    "        batch_dice_fg = dice_score_foreground(outputs, masks)\n",
    "        total_dice_fg += batch_dice_fg.item()\n",
    "        num_batches += 1\n",
    "\n",
    "avg_dice_fg = total_dice_fg / num_batches\n",
    "print(f\"Average Dice Score for Foreground (Classes 1 & 2): {avg_dice_fg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
